{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOop7m6pE1VA2m8Tw4oLAaP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raphaelreinauer/NMT/blob/master/Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5suO0eYeCB75",
        "colab_type": "text"
      },
      "source": [
        "**Implementing a Neural Machine Translation Model**\n",
        "\n",
        "We will use the encoder/decoder architecture. With GRU layers for both encoder and decoder as well as a Dense layer, a RepeatVector layer, and a TimeDistributed layer.\n",
        "\n",
        "Compilation Time: 6 days on 96 GPUs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5LdsvN7CmSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GRU, Dense, TimeDistributed, RepeatVector, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BdyT_BwGiXx",
        "colab_type": "text"
      },
      "source": [
        "**Preprocessing**\n",
        "\n",
        "\n",
        "en_text: english text\n",
        "\n",
        "fr_text: french translation of english text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REY4IAo7GvBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load files\n",
        "#!wget https://assets.datacamp.com/production/repositories/4609/datasets/3459f954752fb2fce7c0b29e25f067e9784b69fb/vocab_en.txt\n",
        "#!wget https://assets.datacamp.com/production/repositories/4609/datasets/644e461abb0910edb038e8b2c4ce7071b5aeca12/vocab_fr.txt\n",
        "\n",
        "#!wget https://raw.githubusercontent.com/udacity/deep-learning/master/language-translation/data/small_vocab_en\n",
        "#!wget https://raw.githubusercontent.com/udacity/deep-learning/master/language-translation/data/small_vocab_fr\n",
        "\n",
        "\n",
        "\n",
        "with open('small_vocab_en', 'r') as en_text_file:\n",
        "    en_text=en_text_file.readlines()\n",
        "\n",
        "with open('small_vocab_fr', 'r') as fr_text_file:\n",
        "    fr_text=fr_text_file.readlines()\n",
        "\n",
        "\n",
        "en_len = 15\n",
        "fr_len = 25\n",
        "en_vocab = 100\n",
        "fr_vocab = 125\n",
        "\n",
        "\n",
        "# english tokenizer\n",
        "en_tok = Tokenizer(num_words=100) #only consider num_words most comman words the other words will be considered as out-of-vocabulary (OOV)\n",
        "\n",
        "en_tok.fit_on_texts(en_text)\n",
        "\n",
        "# french tokenizer\n",
        "\n",
        "fr_tok = Tokenizer(num_words=125)\n",
        "\n",
        "fr_tok.fit_on_texts(fr_text)\n",
        "\n",
        "'''\n",
        "Tokenizer class as functions:\n",
        "index_word\n",
        "word_index\n",
        "texts_to_sequences: sentence is converted to a list of IDs\n",
        "'''\n",
        "\n",
        "# Padding: make all sentences to be of the same length\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "def sents2seqs(input_type, sentences, onehot=False, pad_type='post', reverse=False):\n",
        "    '''\n",
        "    Converts the sentences to a list of sequence of IDs,\n",
        "    Pad the sentences so that they have equal length and,\n",
        "    Optionally convert the IDs to onehot vectors.\n",
        "    '''\n",
        "    assert input_type in [\"source\", \"target\"]\n",
        "    if input_type == 'source':\n",
        "      tokenizer = en_tok\n",
        "      pad_length = en_len\n",
        "      vocab_size = en_vocab\n",
        "    elif input_type == 'target':\n",
        "      tokenizer = fr_tok\n",
        "      pad_length = fr_len\n",
        "      vocab_size = fr_vocab\n",
        "    \n",
        "    encoded_text = tokenizer.texts_to_sequences(sentences)\n",
        "    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=pad_length)\n",
        "    if reverse:\n",
        "      preproc_text = preproc_text[:,::-1]\n",
        "      \n",
        "    if onehot:\n",
        "        assert vocab_size is not None, \"Cannot do to_categorical without num_classes for safety\"\n",
        "        preproc_text = to_categorical(preproc_text, num_classes=vocab_size)\n",
        "    return preproc_text\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOK7-NfGr9Lc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2b796466-e91f-43ea-c5b7-41b9ce727341"
      },
      "source": [
        "en_vocab"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1aIun5fL4oN",
        "colab_type": "text"
      },
      "source": [
        "**Define the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_Cy-xfvCtaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "baeed506-b1f0-4e18-841c-f87716b68a58"
      },
      "source": [
        "'''\n",
        "# Constants\n",
        "fr_vocab: size of the french vocabulary\n",
        "en_vocab: size of the english vocabulary\n",
        "hsize: size of the hidden state\n",
        "\n",
        "# Different layers\n",
        "en_inputs:encoder input layer\n",
        "en_out and en_state: encoder GRU output\n",
        "de_out decoder: GRU output\n",
        "de_pred decoder: prediction\n",
        "\n",
        "'''\n",
        "\n",
        "hsize = 48\n",
        "\n",
        "\n",
        "# Define the encoder\n",
        "en_inputs = Input(shape=(en_len, en_vocab))\n",
        "en_gru = GRU(hsize, return_state=True)\n",
        "en_out, en_state = en_gru(en_inputs)\n",
        "\n",
        "# Define the encoder model\n",
        "#encoder = Model(inputs=en_inputs, outputs=en_state)\n",
        "\n",
        "# Define the decoder\n",
        "de_inputs = RepeatVector(fr_len)(en_state)\n",
        "de_gru = GRU(hsize, return_sequences=True)\n",
        "de_out = de_gru(de_inputs, initial_state=en_state)\n",
        "\n",
        "# prediction layer\n",
        "de_dense = Dense(fr_vocab, activation='softmax')\n",
        "de_dense_time = TimeDistributed(de_dense)\n",
        "\n",
        "de_pred = de_dense_time(de_out)\n",
        "\n",
        "# Define the complete model\n",
        "nmt = Model(inputs=en_inputs, outputs=de_pred)\n",
        "\n",
        "# Compile the model\n",
        "nmt.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "nmt.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 15, 100)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gru (GRU)                       [(None, 48), (None,  21600       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector (RepeatVector)    (None, 25, 48)       0           gru[0][1]                        \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     (None, 25, 48)       14112       repeat_vector[0][0]              \n",
            "                                                                 gru[0][1]                        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 25, 125)      6125        gru_1[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 41,837\n",
            "Trainable params: 41,837\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nTj4aqJDvAC",
        "colab_type": "text"
      },
      "source": [
        "**Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmvWv49qVy8t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "66042f5d-53bc-45f9-f083-230daedea7b5"
      },
      "source": [
        "en_text = en_text[:1000]\n",
        "fr_text = fr_text[:1000]\n",
        "n_epochs = 5\n",
        "bsize = 250\n",
        "data_size = len(en_text)\n",
        "\n",
        "train_size, valid_size = 800, 200\n",
        "inds = np.arange(len(en_text))\n",
        "np.random.shuffle(inds)\n",
        "\n",
        "\n",
        "train_inds = inds[:train_size]\n",
        "valid_inds = inds[train_size:train_size+valid_size]\n",
        "\n",
        "tr_en = [en_text[ti] for ti in train_inds]\n",
        "tr_fr = [fr_text[ti] for ti in train_inds]\n",
        "v_en = [en_text[ti] for ti in valid_inds]\n",
        "v_fr = [fr_text[ti] for ti in valid_inds]\n",
        "\n",
        "v_en_x = sents2seqs('source', v_en, onehot=True, reverse=True)\n",
        "v_de_y = sents2seqs('target', v_fr, onehot=True)\n",
        "\n",
        "\n",
        "\n",
        "for ei in range(n_epochs):\n",
        "    for i in range(0, data_size, bsize):\n",
        "        en_x = sents2seqs('source', en_text[i:i+bsize], onehot=True, reverse=True)\n",
        "        de_y = sents2seqs('target', fr_text[i:i+bsize], onehot=True)\n",
        "        nmt.train_on_batch(en_x, de_y)\n",
        "\n",
        "    # Evaluate the trained model on the validation data\n",
        "    res = nmt.evaluate(v_en_x, v_de_y, batch_size=valid_size, verbose=0)\n",
        "\n",
        "    print(\"{} => Loss:{}, Val Acc: {}\".format(ei+1,res[0], res[1]*100.0))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 => Loss:4.762917995452881, Val Acc: 47.02000021934509\n",
            "2 => Loss:4.6717329025268555, Val Acc: 53.56000065803528\n",
            "3 => Loss:4.514949798583984, Val Acc: 53.53999733924866\n",
            "4 => Loss:4.225861072540283, Val Acc: 53.53999733924866\n",
            "5 => Loss:3.6791574954986572, Val Acc: 53.53999733924866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp8_pSOCE3BO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Training (EN):\\n', tr_en[:3], '\\nTraining (FR):\\n', tr_fr[:3])\n",
        "print('\\nValid (EN):\\n', v_en[:3], '\\nValid (FR):\\n', v_fr[:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AojQTnxzrVhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make Translation\n",
        "\n",
        "en_st = ['the united states is sometimes chilly during december , but it is sometimes freezing in june .']\n",
        "print('English: {}'.format(en_st))\n",
        "\n",
        "# Convert the English sentence to a sequence\n",
        "en_seq = sents2seqs('source', en_st, onehot=True, reverse=True)\n",
        "\n",
        "# Predict probabilities of words using en_seq\n",
        "fr_pred = model.predict(en_seq)\n",
        "\n",
        "# Get the sequence indices (max argument) of fr_pred\n",
        "fr_seq = np.argmax(fr_pred, axis=-1)[0]\n",
        "\n",
        "# Convert the sequence of IDs to a sentence and print\n",
        "fr_sent = [fr_tok.index_word[i] for i in fr_seq if i != 0]\n",
        "print(\"French (Custom): {}\".format(' '.join(fr_sent)))\n",
        "print(\"French (Google Translate): les etats-unis sont parfois froids en décembre, mais parfois gelés en juin\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}